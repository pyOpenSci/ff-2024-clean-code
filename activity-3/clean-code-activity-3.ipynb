{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5038f3a5-6c74-441c-9d96-a5f711543563",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "(clean-code-activity-3)=\n",
    "# Tests & Checks for your code: Activity 3\n",
    "\n",
    "* In [activity 1](../activity-1/clean-code-activity-1), you made your code cleaner and more usable using [expressive variable names](python-expressive-code) and docstrings to document the module. \n",
    "* In [activity 2](../activity-2/clean-code-activity-2), you made your code more DRY (\"Don't Repeat Yourself\") using [functions](write-functions) and [conditionals](conditionals). \n",
    "\n",
    "In this activity, you will build checks into your workflow using [try/except](try-except) blocks added to functions to handle some \"features\" found in the JOSS, CrossRef citation data.\n",
    "\n",
    ":::{note}\n",
    "A data feature, as defined here, represents unexpected values that may be found in real-world data. You will rarely find that your data can be processed without some cleaning steps! \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ba0c5d-36ce-4859-a57a-25e05bf5a0d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Real world data processing & workflows and edge cases \n",
    "\n",
    "Real-world data can rarely be imported without cleanup steps. You will often find unusual data values you don't expect. Sometimes, these values are documented--for example, a `9999` may represent a missing value in a dataset. And sometimes, that missing value is documented for you. Yay! \n",
    "\n",
    "Other times, the data contains undocumented typos and other errors that you need to handle in your code. In this activity, you will see these unusual values referred to as data \"edge cases.\"  \n",
    "\n",
    "Writing robust code that handles unexpected values will make your code run smoothly and fail gracefully. This type of code, which combines functions (or classes) and checks within the functions that handle messy data, will make your code easier to maintain.\n",
    "\n",
    "### Strategies for handling messy data\n",
    "\n",
    "There are several strategies that you can employ to handle unusual data values. In this activity, you will apply the following strategies to make your code more robust, maintainable & usable:\n",
    "\n",
    "* **[conditional statements](../../code-workflow-logic/python-conditionals)** \n",
    "  to check for specific conditions before executing code. This allows you to create different pathways for code to execute based on specific conditions.\n",
    "* **[try/except blocks](../../code-workflow-logic/python-function-checks)** allow\n",
    "  you to handle potential errors by attempting an operation and catching any \n",
    "  exceptions if they occur, providing useful feedback. Sometimes, you may want the program to end on an error. In other cases, you may want to handle it in a specific way.\n",
    "* **[fail fast with useful error messages](fail-fast)**: Failing fast is a software engineering term that means allowing your\n",
    "  code to stop when something goes wrong, ensuring that errors are caught \n",
    "  and communicated promptly. This helps the user quickly understand the error, what went \n",
    "  wrong, and where.\n",
    "\n",
    ":::{tip}\n",
    "As you make decisions about adding checks to your code, weigh the value of using [Pythonic approach](pythonic-checks) vs. literal checks (look before you leap) to address potential errors in your code. This means asking yourself if the code should ask for forgiveness later or check an object's state or type before proceeding.\n",
    ":::\n",
    "\n",
    "<!-- Based on some of Carol's comments, maybe we focus less on what is Pythonic and more on the critical thinking element surrounding when to use conditionals vs. a try/except approach. I don't think that is clear yet in these lessons.\n",
    " -->\n",
    " \n",
    "### Functions, classes, and methods are a tool\n",
    "\n",
    "Using functions and class methods is a great first step in handling messy data. A function or method provides a modular unit you can test outside the workflow for the edge cases you may encounter. Also, because a function is a modular unit, you can add elements to handle unexpected processing features as you build your workflow.\n",
    "\n",
    "Once you have these functions and methods, you can add checks using conditional statements and [try/except](try-except) blocks that anticipate edge cases and errors you may encounter when processing your data. \n",
    "\n",
    "<!-- Consider if this last paragraph might belong in the function checks lesson vs. here. \n",
    "\n",
    "## Clean up your workflow\n",
    "\n",
    "The code below is an example of what your code might look like after completing [Activity 2](activity-2). Notice that the code below fails when run.\n",
    " \n",
    ":::{admonition} What's changed in the data?\n",
    ":class: tip\n",
    "\n",
    "In this workflow, you have a new set of data files to open in your list of `.json` files. The first file, has some unexpected \"features\" that your code needs to handle gracefully to process all of the data successfully.\n",
    "\n",
    ":::\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926989e3-fbe9-47fa-932d-e001c5416fd4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "source": [
    "## Activity 3, part 1: Find the data & fail fast when it's missing\n",
    "\n",
    "If you are processing specific data in your workflow, then ensuring your code can successfully find the data is your first (and possibly most important) goal. \n",
    "\n",
    "**Consider:** How does your code handle and tell a user that it can't find the data that you want it to open?\n",
    "\n",
    "If your code doesn't [fail fast](fail-fast) with a useful error message, and it continues to run and fails later, it will potentially confuse a user. The error that will likely be raised later will likely not alert the user that the issue is actually missing data vs something else. \n",
    "\n",
    "This will then mislead someone when trying to troubleshoot your code. \n",
    "\n",
    "### Activity 3, part 1 code example\n",
    "\n",
    "Consider the code below. Note that the code below has an incorrect `/data` directory path that doesn't exist. Notice that the error that is thrown after running the code is not a [`FileNotFounderror`](file_error). \n",
    "\n",
    "Instead, it raises a [`ValueError`](value_error): `ValueError: No objects to concatenate`), which is much less useful to a user (who could be your future self).\n",
    "\n",
    "\n",
    ":::{admonition} Group work\n",
    ":class: warning\n",
    "\n",
    "In small groups, consider the code and answer the following questions together.\n",
    "\n",
    "Questions:\n",
    "\n",
    "* Does the code fail fast?\n",
    "* What type of error do you want Python to throw when it can't find a data file? Use Google, LLMs, or our [tests and checks](common-exceptions) lesson to help figure this out. \n",
    "* Does the code handle the actual error gracefully?\n",
    "* How can you make the code better handle missing data files?\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90826f78-99e8-4c1a-b88f-1a94af0136e5",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     papers_df \u001b[38;5;241m=\u001b[39m load_clean_json(json_file, columns_to_keep)\n\u001b[1;32m     35\u001b[0m     all_papers_list\u001b[38;5;241m.\u001b[39mappend(papers_df)\n\u001b[0;32m---> 37\u001b[0m all_papers_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_papers_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/lessons/lib/python3.11/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/mambaforge/envs/lessons/lib/python3.11/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/mambaforge/envs/lessons/lib/python3.11/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_clean_json(file_path, columns_to_keep):\n",
    "    \"\"\"\n",
    "    Load JSON data from a file. Drop unnecessary columns and normalize\n",
    "    to DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : Path\n",
    "        Path to the JSON file.\n",
    "    columns_to_keep : list\n",
    "        List of columns to keep in the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Loaded JSON data.\n",
    "    \"\"\"\n",
    "\n",
    "    with file_path.open(\"r\") as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    return pd.json_normalize(json_data)\n",
    "\n",
    "# Notice that this is bad data dir\n",
    "# What happens when your code runs?\n",
    "data_dir = Path(\"bad-bad-data\")\n",
    "\n",
    "all_papers_list = []\n",
    "for json_file in data_dir.glob(\"*.json\"):\n",
    "    papers_df = load_clean_json(json_file, columns_to_keep)\n",
    "    all_papers_list.append(papers_df)\n",
    "\n",
    "all_papers_df = pd.concat(all_papers_list, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871ff899-36a5-4bc6-8621-aa6896ee1097",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Activity 3, part 2: Add checks to the `format_date` function\n",
    "\n",
    "The code below creates a {class}`pandas.DataFrame` with the first 15 publications in the JOSS sample `data.json` file. This is the first of 3 files you must process in your workflow.\n",
    "\n",
    "Your first task is to process and format the `published_date` column in the data to make it a {class}`pandas.Timestamp` object. Having a date in a `datetime` format like {class}`pandas.Timestamp` or {class}`datetime.datetime` will allow you to do time-related analysis on your data, such as counting publications by month and year! The expected CrossRef published date should be:\n",
    "\n",
    "```json\n",
    "\"published\": {\n",
    "      \"date-parts\": [\n",
    "        [\n",
    "          2022,\n",
    "          11,\n",
    "          27\n",
    "        ]\n",
    "      ]\n",
    "    }\n",
    "```\n",
    "\n",
    "However, the date is not always formatted as expected in the above sample data.\n",
    "\n",
    "For this activity, focus on adding checks to the `format_date` function. **IMPORTANT:** Use the sample data provided below for your troubleshooting exercise. This will allow you to focus on fixing only one function rather than trying to troubleshoot the entire workflow!  \n",
    "\n",
    "\n",
    ":::{admonition} Activity 2: Part 2\n",
    ":class: warning \n",
    "\n",
    "In small groups, do the following:\n",
    "\n",
    "1. Evaluate the `published_date` field in the data created below and answer the question: \n",
    "\n",
    "* Do you see any unusually-formatted values that may be responsible for making your code above fail?\n",
    "\n",
    "2. Once you have a list of issues you observe in the data, address them by modifying the `format_date` function below.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0493239-b8a4-49e7-a0b5-3ff11396ac8c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output",
     "hide-cell"
    ]
   },
   "source": [
    "###  Format dates with {func}`pandas.to_datetime`\n",
    "\n",
    "Let's work on formatting dates so there is a consistent format in our dataframe. Python has a [string formatting language](https://docs.python.org/3/library/string.html#formatspec) that defines useful characters for formatting.\n",
    "\n",
    "\n",
    "What Does `02d` Mean?\n",
    "\n",
    "* `d`: This part of the format code means you’re expecting an integer. It tells Python to format the value as a decimal (whole number).\n",
    "* `02`: The `02` means the number should be padded with leading zeros if necessary, so the total width is 2 digits. For example:\n",
    "\n",
    "* `1` becomes `01`\n",
    "* `5` becomes `05`\n",
    "* `12` stays as `12` (no padding needed)\n",
    "\n",
    "This is especially useful for formatting months or days, which often require a `MM-DD` format (e.g., 01-05 for January 5th)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "70245af2-533d-47ff-b4c4-b16467b27e83",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>citations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bmiptools: BioMaterials Image Processing Tools]</td>\n",
       "      <td>[[2022, 11, 27]]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[QuasinormalModes.jl: A Julia package for com...</td>\n",
       "      <td>[2022, 5, 25]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CWInPy: A Python package for inference with c...</td>\n",
       "      <td>[[2022, 9, 29]]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Nempy: A Python package for modelling the Aus...</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Spectral Connectivity: a python package for c...</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[SEEDPOD Ground Risk: A Python application and...</td>\n",
       "      <td>[[2022, 3, ]]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[DIANNA: Deep Insight And Neural Network Analy...</td>\n",
       "      <td>[[2022, 12, 15]]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[diman: A Clojure Package for Dimensional Ana...</td>\n",
       "      <td>[[2022, 1]]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[PERFORM: A Python package for developing redu...</td>\n",
       "      <td>[[9999]]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[TLViz: Visualising and analysing tensor decom...</td>\n",
       "      <td>[[2022, 11, 25]]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[ALUES: R package for Agricultural Land Use Ev...</td>\n",
       "      <td>[[2022, 5, 12]]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[[Spiner: Performance Portable Routines for Ge...</td>\n",
       "      <td>[[2022, 7, 5]]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[pyndl: Naïve Discriminative Learning in Python]</td>\n",
       "      <td>[[2022, 12, 15]]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[HostPhot: global and local photometry of gala...</td>\n",
       "      <td>[[2022, 8, 15]]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[QMKPy: A Python Testbed for the Quadratic Mul...</td>\n",
       "      <td>[[2022, 11, 2]]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title    published_date  \\\n",
       "0    [bmiptools: BioMaterials Image Processing Tools]  [[2022, 11, 27]]   \n",
       "1   [[QuasinormalModes.jl: A Julia package for com...     [2022, 5, 25]   \n",
       "2   [CWInPy: A Python package for inference with c...   [[2022, 9, 29]]   \n",
       "3   [Nempy: A Python package for modelling the Aus...              [[]]   \n",
       "4   [Spectral Connectivity: a python package for c...              [[]]   \n",
       "5   [SEEDPOD Ground Risk: A Python application and...     [[2022, 3, ]]   \n",
       "6   [DIANNA: Deep Insight And Neural Network Analy...  [[2022, 12, 15]]   \n",
       "7   [[diman: A Clojure Package for Dimensional Ana...       [[2022, 1]]   \n",
       "8   [PERFORM: A Python package for developing redu...          [[9999]]   \n",
       "9   [TLViz: Visualising and analysing tensor decom...  [[2022, 11, 25]]   \n",
       "10  [ALUES: R package for Agricultural Land Use Ev...   [[2022, 5, 12]]   \n",
       "11  [[Spiner: Performance Portable Routines for Ge...    [[2022, 7, 5]]   \n",
       "12   [pyndl: Naïve Discriminative Learning in Python]  [[2022, 12, 15]]   \n",
       "13  [HostPhot: global and local photometry of gala...   [[2022, 8, 15]]   \n",
       "14  [QMKPy: A Python Testbed for the Quadratic Mul...   [[2022, 11, 2]]   \n",
       "\n",
       "    citations  \n",
       "0           2  \n",
       "1           2  \n",
       "2           3  \n",
       "3           2  \n",
       "4           3  \n",
       "5           1  \n",
       "6           1  \n",
       "7           0  \n",
       "8           3  \n",
       "9           2  \n",
       "10          1  \n",
       "11          0  \n",
       "12          0  \n",
       "13          1  \n",
       "14          0  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Manually recreate data for the first 15 crossref entries\n",
    "joss_pubs = [\n",
    "    {\n",
    "        \"title\": [\"bmiptools: BioMaterials Image Processing Tools\"],\n",
    "        \"published_date\": [[\"2022\", \"11\", \"27\"]],\n",
    "        \"citations\": 2\n",
    "    },\n",
    "    {\n",
    "        \"title\": [[\"QuasinormalModes.jl: A Julia package for computing discrete eigenvalues of second order ODEs\"]],\n",
    "        \"published_date\": [2022, \"5\", 25],\n",
    "        \"citations\": 2\n",
    "    },\n",
    "    {\n",
    "        \"title\": [\"CWInPy: A Python package for inference with continuous gravitational-wave signals from pulsars\"],\n",
    "        \"published_date\": [[2022, 9, \"29\"]],\n",
    "        \"citations\": 3\n",
    "    },\n",
    "    {\n",
    "        \"title\": [\"Nempy: A Python package for modelling the Australian National Electricity Market dispatch procedure\"],\n",
    "        \"published_date\": [[\"\"]],\n",
    "        \"citations\": 2\n",
    "    },\n",
    "    {\n",
    "        \"title\": [\"Spectral Connectivity: a python package for computing spectral coherence and related measures\"],\n",
    "        \"published_date\": [[]],  # No date available\n",
    "        \"citations\": 3\n",
    "    },\n",
    "    {\n",
    "        \"title\": [\"SEEDPOD Ground Risk: A Python application and framework for assessing the risk to people on the ground from uncrewed aerial vehicles (UAVs)\"],\n",
    "        \"published_date\": [[\"2022\", \"3\", \"\"]],\n",
    "        \"citations\": 1\n",
    "    },\n",
    "    {\n",
    "        \"title\": [\"DIANNA: Deep Insight And Neural Network Analysis, explainability in time series\"],\n",
    "        \"published_date\": [[2022, 12, 15]],\n",
    "        \"citations\": 1\n",
    "    },\n",
    "    {\n",
    "        \"title\": [[\"diman: A Clojure Package for Dimensional Analysis and Unit Checking\"]],\n",
    "        \"published_date\": [[2022, 1]],\n",
    "        \"citations\": 0\n",
    "    },\n",
    "    {\n",
    "        \"title\": [\"PERFORM: A Python package for developing reduced-order models for flow simulation\"],\n",
    "        \"published_date\": [[9999]],\n",
    "        \"citations\": 3\n",
    "    },\n",
    "    {\n",
    "        \"title\": [\"TLViz: Visualising and analysing tensor decompositions\"],\n",
    "        \"published_date\": [[2022, 11, 25]],\n",
    "        \"citations\": 2\n",
    "    },\n",
    "    {\n",
    "        \"title\": [\"ALUES: R package for Agricultural Land Use Evaluation System\"],\n",
    "        \"published_date\": [[2022, 5, 12]],\n",
    "        \"citations\": 1\n",
    "    },\n",
    "    {\n",
    "        \"title\": [[\"Spiner: Performance Portable Routines for Generalized SpMV and Triangular Solvers\"]],\n",
    "        \"published_date\": [[2022, 7, 5]],\n",
    "        \"citations\": 0\n",
    "    },\n",
    "    {\n",
    "        \"title\": [\"pyndl: Naïve Discriminative Learning in Python\"],\n",
    "        \"published_date\": [[2022, 12, 15]],\n",
    "        \"citations\": 0\n",
    "    },\n",
    "    {\n",
    "        \"title\": [\"HostPhot: global and local photometry of galaxies\"],\n",
    "        \"published_date\": [[2022, 8, 15]],\n",
    "        \"citations\": 1\n",
    "    },\n",
    "    {\n",
    "        \"title\": [\"QMKPy: A Python Testbed for the Quadratic Multichannel Kalman Filter\"],\n",
    "        \"published_date\": [[2022, 11, 2]],\n",
    "        \"citations\": 0\n",
    "    }\n",
    "]\n",
    "\n",
    "joss_pubs_df = pd.DataFrame(joss_pubs)\n",
    "joss_pubs_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e2ea9e7-8322-45c3-af3d-3d350b44c53f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_date(date_parts: list) -> str:\n",
    "    \"\"\"\n",
    "    Format date parts into a string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    date_parts : list\n",
    "        List containing year, month, and day.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.datetime\n",
    "        A date formatted as a pd.datetime object.\n",
    "    \"\"\"\n",
    "    # A print statement might help you identify the issue\n",
    "    print(f\"The input value is: {date_parts}\")\n",
    "    date_str = (\n",
    "        f\"{date_parts[0][0]}-{date_parts[0][1]:02d}-{date_parts[0][2]:02d}\"\n",
    "    )\n",
    "    return pd.to_datetime(date_str, format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6f84a48-bcc1-4de1-9683-37cf061acf81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2022', '11', '27']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joss_pubs_df[\"published_date\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fe34990-1398-4929-9c24-083ad010b03f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input value is: [[2022, 9, '29']]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown format code 'd' for object of type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Format date fails on row 3\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mformat_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoss_pubs_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpublished_date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 18\u001b[0m, in \u001b[0;36mformat_date\u001b[0;34m(date_parts)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# A print statement might help you identify the issue\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input value is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_parts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m date_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_parts[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_parts[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_parts[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mto_datetime(date_str, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown format code 'd' for object of type 'str'"
     ]
    }
   ],
   "source": [
    "# Format date fails on row 3\n",
    "format_date(joss_pubs_df[\"published_date\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "194a3f0a-2d0b-4da5-966b-be2cc7b6b946",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input value is: [[2022, 8, 15]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2022-08-15 00:00:00')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format date runs fine on row 14\n",
    "format_date(joss_pubs_df[\"published_date\"][13])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531fb86-5690-4122-9b60-b81e4bb5bb12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### How to apply functions to DataFrame values: `.apply()` \n",
    "\n",
    "The {meth}`.apply() <pandas.DataFrame.apply>` method allows you to apply any function to rows or columns in a {class}`pandas.DataFrame`. For example, you can use it to perform operations on specific column or row values. When you use `.apply()`, you can specify whether you want to apply the function across columns `(axis=0)` (the default) or across rows `(axis=1)`. \n",
    "\n",
    "For example, if you want to apply a function to each row of a DataFrame, you would use `df.apply(your_function, axis=1)`. This function is especially useful for applying logic that can’t be easily achieved with built-in pandas functions, allowing for more flexibility in data processing.\n",
    "\n",
    "You can use `.apply` in pandas to efficiently replace `for loops` to process row and column values in a {class}`pandas.DataFrame`.\n",
    "\n",
    "```python\n",
    "# Apply the format_date function to every row in the published_date column\n",
    "joss_pubs_df['published_date'].apply(format_date)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c2b00f-6c7f-4a5e-971b-57fe4cbc8f17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{tip}\n",
    "* If you are using Jupyter, then you might [find this page helpful when setting up debugging.](https://jupyterlab.readthedocs.io/en/stable/user/debugger.html)\n",
    "* VSCODE has a nice visual debugger that you can use.\n",
    ":::\n",
    "\n",
    "Important: It is ok if you can't get the code to run fully by the end of this workshop. If you can:\n",
    "\n",
    "1. identify at least one of the data processing \"bugs\" (even if you can't fix it) and/or\n",
    "2. fix at least one bug\n",
    "\n",
    "You can consider your effort today as a success!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3633a7-d531-4614-8c93-c5af89c0b87a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Activity 3, part 3 \n",
    "\n",
    "<!-- This activity might be simpler than the date one?  -->\n",
    "\n",
    ":::{admonition} Activity 3.3\n",
    "\n",
    "Your goal in this activity is to generate a list of all package names \n",
    "found in the example CrossRef data. Below is a `clean_title` function \n",
    "and a small workflow that parses through all titles in the sample data. \n",
    "\n",
    "However, the function isn't working as expected. Add checks to \n",
    "the `clean_title` function to ensure it correctly extracts the title of each \n",
    "package in each publication. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ea715b60-a370-4924-b677-fe5f41defa5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [bmiptools: BioMaterials Image Processing Tools]\n",
       "1     [[QuasinormalModes.jl: A Julia package for com...\n",
       "2     [CWInPy: A Python package for inference with c...\n",
       "3     [Nempy: A Python package for modelling the Aus...\n",
       "4     [Spectral Connectivity: a python package for c...\n",
       "5     [SEEDPOD Ground Risk: A Python application and...\n",
       "6     [DIANNA: Deep Insight And Neural Network Analy...\n",
       "7     [[diman: A Clojure Package for Dimensional Ana...\n",
       "8     [PERFORM: A Python package for developing redu...\n",
       "9     [TLViz: Visualising and analysing tensor decom...\n",
       "10    [ALUES: R package for Agricultural Land Use Ev...\n",
       "11    [[Spiner: Performance Portable Routines for Ge...\n",
       "12     [pyndl: Naïve Discriminative Learning in Python]\n",
       "13    [HostPhot: global and local photometry of gala...\n",
       "14    [QMKPy: A Python Testbed for the Quadratic Mul...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joss_pubs_df[\"title\"].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cd0d2e50-c91f-48f2-9509-fd0e5cb371ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_title(title):\n",
    "    \"\"\"Get package name from a crossref title string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    title : str\n",
    "        The title string containing a package name followed by a colon and description.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The package name before the colon.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    return title[0].split(':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f3531ea2-4317-40dd-9205-545d7c3abae2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m all_titles \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a_title \u001b[38;5;129;01min\u001b[39;00m joss_pubs_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m----> 4\u001b[0m     all_titles\u001b[38;5;241m.\u001b[39mappend(\u001b[43mclean_title\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_title\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m all_titles\n",
      "Cell \u001b[0;32mIn[80], line 16\u001b[0m, in \u001b[0;36mclean_title\u001b[0;34m(title)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_title\u001b[39m(title):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get package name from a crossref title string.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtitle\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# Add checks to the clean_title function to make sure this code runs\n",
    "all_titles = []\n",
    "for a_title in joss_pubs_df[\"title\"]:\n",
    "    all_titles.append(clean_title(a_title))\n",
    "all_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f7e018d8-c9c2-4252-86e7-07e684f4eaae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bmiptools', ' BioMaterials Image Processing Tools']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = joss_pubs_df[\"title\"][0]\n",
    "a[0].split(':')\n",
    "#joss_pubs_df[\"title\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "63845db5-fe92-4da6-b349-294b583e0f48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bmiptools: BioMaterials Image Processing Tools']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# The title value in the first row of the df\n",
    "print(joss_pubs_df[\"title\"][0])\n",
    "print(type(joss_pubs_df[\"title\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00b0733c-2817-4a09-b6b3-f81306f380ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bmiptools: BioMaterials Image Processing Tools\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# The title value unnested from the list\n",
    "print(joss_pubs_df[\"title\"][0][0])\n",
    "print(type(joss_pubs_df[\"title\"][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ac54ceb-0fdb-43cf-9e2f-dd044380afff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value is ['bmiptools: BioMaterials Image Processing Tools']\n",
      "bmiptools: BioMaterials Image Processing Tools\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bmiptools', ' BioMaterials Image Processing Tools']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"The value is {joss_pubs_df['title'][0]}\")\n",
    "get_title(joss_pubs_df[\"title\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca8bfcfe-f502-4dbf-b9eb-3c87887c525b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi [['QuasinormalModes.jl: A Julia package for computing discrete eigenvalues of second order ODEs']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['QuasinormalModes.jl: A Julia package for computing discrete eigenvalues of second order ODEs']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_title(joss_pubs_df[\"title\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ca4c7-343e-4071-9902-7f6b467bea08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## On your own\n",
    "\n",
    ":::{admonition} On Your Own 1 \n",
    "\n",
    "If you complete all the activities above, consider this challenge. \n",
    "Fix the workflow below so it runs. To do this, you can use the results of the functions you worked on above. \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cbb7555-7280-4087-af52-27c37a57b09c",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/2022-01-joss-publications.json\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown format code 'd' for object of type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(json_file)\n\u001b[1;32m     73\u001b[0m papers_df \u001b[38;5;241m=\u001b[39m load_clean_json(json_file, columns_to_keep)\n\u001b[0;32m---> 74\u001b[0m papers_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublished_date\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpapers_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpublished.date-parts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformat_date\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m papers_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m papers_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_title)\n\u001b[1;32m     80\u001b[0m all_papers_list\u001b[38;5;241m.\u001b[39mappend(papers_df)\n",
      "File \u001b[0;32m~/mambaforge/envs/lessons/lib/python3.11/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/lessons/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/lessons/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/mambaforge/envs/lessons/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/lessons/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[2], line 47\u001b[0m, in \u001b[0;36mformat_date\u001b[0;34m(date_parts)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_date\u001b[39m(date_parts: \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    Format date parts into a string.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m        A date formatted as a pd.datetime object.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     date_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_parts[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_parts[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_parts[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mto_datetime(date_str, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown format code 'd' for object of type 'str'"
     ]
    }
   ],
   "source": [
    "# Full code snippet\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_clean_json(file_path, columns_to_keep):\n",
    "    \"\"\"\n",
    "    Load JSON data from a file. Drop unnecessary columns and normalize\n",
    "    to DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : Path\n",
    "        Path to the JSON file.\n",
    "    columns_to_keep : list\n",
    "        List of columns to keep in the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Loaded JSON data.\n",
    "    \"\"\"\n",
    "\n",
    "    with file_path.open(\"r\") as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    normalized_data = pd.json_normalize(json_data)\n",
    "\n",
    "    return normalized_data.filter(items=columns_to_keep)\n",
    "\n",
    "\n",
    "def format_date(date_parts: list) -> str:\n",
    "    \"\"\"\n",
    "    Format date parts into a string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    date_parts : list\n",
    "        List containing year, month, and day.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.datetime\n",
    "        A date formatted as a `pd.datetime` object.\n",
    "    \"\"\"\n",
    "    date_str = (\n",
    "        f\"{date_parts[0][0]}-{date_parts[0][1]:02d}-{date_parts[0][2]:02d}\"\n",
    "    )\n",
    "    return pd.to_datetime(date_str, format=\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def clean_title(value):\n",
    "    \"\"\"Removes a value contained in a list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    value : list\n",
    "        A list containing one or more elements.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Any\n",
    "        The first element of the list `value`.\n",
    "    \"\"\"\n",
    "    print(\"hi\", value)\n",
    "    return value[0]\n",
    "\n",
    "\n",
    "columns_to_keep = [\n",
    "    \"publisher\",\n",
    "    \"DOI\",\n",
    "    \"type\",\n",
    "    \"author\",\n",
    "    \"is-referenced-by-count\",\n",
    "    \"title\",\n",
    "    \"published.date-parts\",\n",
    "]\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "all_papers_list = []\n",
    "for json_file in sorted(data_dir.glob(\"*.json\")):\n",
    "    print(json_file)\n",
    "    papers_df = load_clean_json(json_file, columns_to_keep)\n",
    "    papers_df[\"published_date\"] = papers_df[\"published.date-parts\"].apply(\n",
    "        format_date\n",
    "    )\n",
    "    papers_df[\"title\"] = papers_df[\"title\"].apply(clean_title)\n",
    "\n",
    "\n",
    "    all_papers_list.append(papers_df)\n",
    "\n",
    "all_papers_df = pd.concat(all_papers_list, axis=0, ignore_index=True)\n",
    "\n",
    "print(\"Final shape of combined DataFrame:\", all_papers_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
